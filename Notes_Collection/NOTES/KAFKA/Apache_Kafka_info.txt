Reference
**********
1)How to launch Git Bash from right-click context menu on Windows 10 
https://www.youtube.com/watch?v=kIgZEdyn1dA

2)Install zookeeper & kafka on windows
https://www.youtube.com/watch?v=YgimJWHvBNY

https://www.youtube.com/watch?v=lFox22RJE7s&list=WL&index=1

3)Install Kafka using Docker
https://www.youtube.com/watch?v=EiMi11slVnY

Apache Kafka
**************************

=> Apache Kafka is the messaging queue
=> In an Microservice based architecture, Micriservice are communicated via messages.

Microservice1  =>  Kafka Broker/Kafka server  => Microservice2
(Producer)										                        (Consumer)


=> Microservice1 sends message to the Kafka Broker/Kafka Server then from Kafka Broker , Microservice2 reads the message


Kafka Architecture
---------------------

Kafka_Cluster
-> Multiple Kafka Broker/Kafka server (Kafka_Cluster should contains atleast 3 Kafka_broker/kafka_server)
-> Multiple kafka_Broker/Kafka_servers are managed by Zookeeper

-> Kafka_Topic => In order to send message/data by Producer and comsumed by Consumer, data need a identification mechanism , so that Consumer can read the message
-> every Kafka_Topic has a unique name


**************************************************************************************************************************************************************
INSTALLATION :-
-------------------

Apache kafka Install -> On windows using Binary :-
----------------------------------------------------------------
----------------------------------------------------------------

Reference = https://dzone.com/articles/running-apache-kafka-on-windows-os

https://kafka.apache.org/quickstart

->at first Zooper-server need to be started
-> after that kafka server need to be started (FYI. Kafka server means Kafka Broker)

Steps -

1)start the zookeeper-server
2)start the kafka-server
3)create a topic to store events
4)write some events on the Topics (Producer)
5)Read events from the Topics (Consumer)


Start/Install Kafka and Zoookeeper ->>>> Windows Command
------------------------------------------------------------

Reference = https://www.youtube.com/watch?v=lFox22RJE7s&list=WL&index=1


Command For Windows 11
----------------------------
zookeeper-server-start.bat ..\..\config\zookeeper.properties

kafka-server-start.bat ..\..\config\server.properties

kafka-topics.bat --create --topic my-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3

kafka-console-producer.bat --broker-list localhost:9092 --topic my-topic

kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic --from-beginning


Commands for Windows 10
---------------------------

Installation Process:
1. Download Kafka (https://kafka.apache.org/downloads)
2. Unzip Kafka and go to Kafka Folder
3. Run Zookeeper: zookeeper-server-start.bat config/zookeeper.properties
4. Run Kafka: Kafka-server-start.bat config/server.properties
5. Create Kafka Topic: kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic Youtube
6. Start Producer:  kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic Youtube
7. Start Consumer: kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic Youtube--from-beginning

*************************************************************************************************************************************************************
*************************************************************************************************************************************************************

commands for start and read topic data by Consumer

1)Start zookeeper service
cd Downloads/kafka
bin/zookeeper-server-start.sh config/zookeeper.properties

2)Start kafka broker
cd Downloads/kafka
bin/kafka-server-start.sh config/server.properties


cd Downloads/kafka
bin/kafka-console-consumer.sh --topic wikimedia_recentchange_topic --from-beginning --bootstrap-server localhost:9092



*************************************************************************************************************************************************************

Kafka Install Using Docker
----------------------------
----------------------------

Reference = https://www.youtube.com/watch?v=EiMi11slVnY

Steps :-
---------
1)create a empty project on Intellij
2)create a docker-compose.yml file (project -> Rht Click -> File -> Docker-compose.yml)
|- 2 services (Zookeeper and Kafka server need to be start via Docker)

3)write docker compose file on Intellij 
File - docker-compose.yml
-------------------------

version: '2.18.1'

services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: localhost
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

4)Run the docker command for running the Docker-compose file

yml file folder location -> CMD -> docker compose -f docker-compose.yml up -d

(Ex - D:\DOCKER_POC\dockerComposeYmlPOC>docker compose -f docker-compose.yml up -d )




*****************************************************************************************************************************************************
*****************************************************************************************************************************************************


***  SPRINGBOOT + KAFKA PRODUCER AND CONSUMER FOR STRING MESSAGE  ***
-----------------------------------------------------------------------

1)Create a Springboot project from SpringInitializer 
(dependency >>> spring web , spring for Apache kafka)


2)Configure Kafka Producer and Consumer - update application.properties file

  File - application.properties

  spring.kafka.consumer.bootstrap-servers: localhost:9092
  spring.kafka.consumer.group-id: myGroup
  spring.kafka.consumer.auto-offset-reset: earliest
  spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  spring.kafka.consumer.value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

  spring.kafka.producer.bootstrap-servers: localhost:9092
  spring.kafka.producer.key-serializer: org.apache.kafka.common.serialization.StringSerializer
  spring.kafka.producer.value-serializer: org.apache.kafka.common.serialization.StringSerializer


3)Create Kafka topic - Create a configuration file for the Topic

File - KafkaTopicConfig.java

  package net.javaguides.springboot.config;

  import org.apache.kafka.clients.admin.NewTopic;
  import org.springframework.context.annotation.Bean;
  import org.springframework.context.annotation.Configuration;
  import org.springframework.kafka.config.TopicBuilder;

  @Configuration
  public class KafkaTopicConfig {

      @Bean
      public NewTopic javaguidesTopic(){
          return TopicBuilder.name("javaguides")
                  .build();
      }
  }



4)Create kafka producer - create a Kafka Producer class

  package net.javaguides.springboot.kafka;

  import org.slf4j.Logger;
  import org.slf4j.LoggerFactory;
  import org.springframework.kafka.core.KafkaTemplate;
  import org.springframework.stereotype.Service;

  @Service
  public class KafkaProducer {

      private static final Logger LOGGER = LoggerFactory.getLogger(KafkaProducer.class);

      private KafkaTemplate<String, String> kafkaTemplate;
      //constrcuctor injection
      public KafkaProducer(KafkaTemplate<String, String> kafkaTemplate) {
          this.kafkaTemplate = kafkaTemplate;
      }

      public void sendMessage(String message){
          LOGGER.info(String.format("Message sent %s", message));
          kafkaTemplate.send("javaguides", message);
      }
  }



5)Create REST API to send message - Create a Controller class for the REST API end point

  package net.javaguides.springboot.controller;

  import net.javaguides.springboot.kafka.KafkaProducer;
  import org.springframework.http.ResponseEntity;
  import org.springframework.web.bind.annotation.GetMapping;
  import org.springframework.web.bind.annotation.RequestMapping;
  import org.springframework.web.bind.annotation.RequestParam;
  import org.springframework.web.bind.annotation.RestController;

  @RestController
  @RequestMapping("/api/v1/kafka")
  public class MessageController {

      private KafkaProducer kafkaProducer;

      public MessageController(KafkaProducer kafkaProducer) {
          this.kafkaProducer = kafkaProducer;
      }

      // http:localhost:8080/api/v1/kafka/publish?message=hello world
      @GetMapping("/publish")
      public ResponseEntity<String> publish(@RequestParam("message") String message){
          kafkaProducer.sendMessage(message);
          return ResponseEntity.ok("Message sent to the topic");
      }
  }


6)Create Kafka Consumer - Create a Consumer class


  package net.javaguides.springboot.kafka;

  import org.slf4j.Logger;
  import org.slf4j.LoggerFactory;
  import org.springframework.kafka.annotation.KafkaListener;
  import org.springframework.stereotype.Service;

  @Service
  public class KafkaConsumer {

      private static final Logger LOGGER = LoggerFactory.getLogger(KafkaConsumer.class);

      @KafkaListener(topics = "javaguides", groupId = "myGroup")
      public void consume(String message){
          LOGGER.info(String.format("Message received -> %s", message));
      }
  }


*************************************************************************************************************************************************************
*************************************************************************************************************************************************************



***   SPRINGBOOT + KAFKA , PRODUCER AND CONSUMER FOR JSON OBJECT (JSON TO JAVA POJO AND VICE VERSA) ****
---------------------------------------------------------------------------------------------------------

1)Create sprigboot project from Spring Initializer (add dependency SpringWeb , Spring for Apache Kafka)


2)Update application.properties file - for Producer and Consumer configuration

File - application.properties

  spring.kafka.consumer.bootstrap-servers: localhost:9092
  spring.kafka.consumer.group-id: myGroup
  spring.kafka.consumer.auto-offset-reset: earliest
  spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  #spring.kafka.consumer.value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  spring.kafka.consumer.value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
  spring.kafka.consumer.properties.spring.json.trusted.packages=*

  spring.kafka.producer.bootstrap-servers: localhost:9092
  spring.kafka.producer.key-serializer: org.apache.kafka.common.serialization.StringSerializer
  # spring.kafka.producer.value-serializer: org.apache.kafka.common.serialization.StringSerializer
  spring.kafka.producer.value-serializer: org.springframework.kafka.support.serializer.JsonSerializer

  spring.kafka.topic.name=javaguides
  spring.kafka.topic-json.name=javaguides_json


3) Create a POJO


4) Create configuration file for Topic

  package net.javaguides.springboot.config;

  import org.apache.kafka.clients.admin.NewTopic;
  import org.springframework.beans.factory.annotation.Value;
  import org.springframework.context.annotation.Bean;
  import org.springframework.context.annotation.Configuration;
  import org.springframework.kafka.config.TopicBuilder;

  @Configuration
  public class KafkaTopicConfig {

      @Value("${spring.kafka.topic.name}")
      private String topicName;

      @Value("${spring.kafka.topic-json.name}")
      private String topicJsonName;

      @Bean
      public NewTopic javaguidesTopic(){
          return TopicBuilder.name(topicName)
                  .build();
      }

      @Bean
      public NewTopic javaguidesJsonTopic(){
          return TopicBuilder.name(topicJsonName)
                  .build();
      }
  }


5)Create a producer

  package net.javaguides.springboot.kafka;

  import net.javaguides.springboot.payload.User;
  import org.slf4j.Logger;
  import org.slf4j.LoggerFactory;
  import org.springframework.beans.factory.annotation.Value;
  import org.springframework.kafka.core.KafkaTemplate;
  import org.springframework.kafka.support.KafkaHeaders;
  import org.springframework.messaging.Message;
  import org.springframework.messaging.support.MessageBuilder;
  import org.springframework.stereotype.Service;

  @Service
  public class JsonKafkaProducer {

      @Value("${spring.kafka.topic-json.name}")
      private String topicJsonName;

      private static final Logger LOGGER = LoggerFactory.getLogger(JsonKafkaProducer.class);

      private KafkaTemplate<String, User> kafkaTemplate;

      public JsonKafkaProducer(KafkaTemplate<String, User> kafkaTemplate) {
          this.kafkaTemplate = kafkaTemplate;
      }

      public void sendMessage(User data){

          LOGGER.info(String.format("Message sent -> %s", data.toString()));

          Message<User> message = MessageBuilder
                  .withPayload(data)
                  .setHeader(KafkaHeaders.TOPIC, topicJsonName)
                  .build();

          kafkaTemplate.send(message);
      }
  }


6)Create a Consumer

package net.javaguides.springboot.kafka;

import net.javaguides.springboot.payload.User;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class JsonKafkaConsumer {
    private static final Logger LOGGER = LoggerFactory.getLogger(JsonKafkaConsumer.class);

    @KafkaListener(topics = "${spring.kafka.topic-json.name}", groupId = "${spring.kafka.consumer.group-id}")
    public void consume(User user){
        LOGGER.info(String.format("Json message recieved -> %s", user.toString()));
    }
}

7) Create a Controller

package net.javaguides.springboot.controller;

import net.javaguides.springboot.kafka.JsonKafkaProducer;
import net.javaguides.springboot.payload.User;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/api/v1/kafka")
public class JsonMessageController {

    private JsonKafkaProducer kafkaProducer;

    public JsonMessageController(JsonKafkaProducer kafkaProducer) {
        this.kafkaProducer = kafkaProducer;
    }

    @PostMapping("/publish")
    public ResponseEntity<String> publish(@RequestBody User user){
        kafkaProducer.sendMessage(user);
        return ResponseEntity.ok("Json message sent to kafka topic");
    }
}



*************************************************************************************************************************************************************
*************************************************************************************************************************************************************

SPRING BOOT + KAFKA , REAL WORLD PROJECT - Kafka Producer Wikimedia
---------------------------------------------------------------------

WIKIMEDIA - Stream Wikimedia to Database
******************************************

Steps -
Kafka Producer will read the Wikimedia Stream and send it to Kafka Broker , which is then read by kafka Consumer from the Kafka Broker

Wikimedia Stream  ->>  Kafka Producer  ->> Kafka Broker/Kafka Srver  ->> Kafka Consumer

Link = https://stream.wikimedia.org/v2/stream/recentchange

>> above mentioned api will read the stream of Wikimedia recent changes


SPRING BOOT + KAFKA - Project Setup :-
-------------------------------------

Kafka-producer-wikimedia(Microservice 1) ->   Kafka-Broker/kafka-server (Messaging system)  ->  kafka-consumer-Database (Microservice 2)
[producer microservice will                                                                       [kafka-consumer read the message(i.e.wikimedia stream   
                                                                                                     ) and save it into DB]
read the Wikimedia Stream and send the 
message to Kafka-Broker] 




kafka-producer-wikimedia Project Setup (Microservice 1) - (PRODUCER)
**********************************************************************

>>>We will be create multi-module springboot project
>>we will create a module project, module project will be create inside the springboot project which is genetreted by spring-initializer
>>In order to create a Module Project, parent-springboot project should contain 
File - pom.xml
add <packaging>pom</packaging> tag on pom.xml file of the parent project (project which is generated from spring-initializer)


1)Create a SpringBoot project using Spring Initializer
    (Add dependency - Spring for Apache Kafka , LOMBOK)
    Artifact - springboot-kafka-real-world-project


2)Create a module on the parent project

    Module1 -> kafka-producer-wikimedia

    parent-project(springboot-kafka-real-world-project) -> add <packeging>pom</packeging> tag on pom.xml (in order to create the multi module project)

    >> After that create a module project

    Right-click on the parent project -> create Module -> kafka-producer-wikimedia(PRODUCER)

    >> update/maven update the parent project


3)Create a package and main class on module application (kafka-producer-wikimedia)

    package com.springboot.kafkaproducerwikimedia;

    import org.springframework.boot.SpringApplication;
    import org.springframework.boot.autoconfigure.SpringBootApplication;

    @SpringBootApplication
    public class SpringbootProducerApplication {

        public static void main(String args[]){
            SpringApplication.run(SpringbootProducerApplication.class);
        }
    }


4)add <packaging>jar</packaging> tag on the pom.xml file of module project (kafka-producer-wikimedia)
>> maven update project/maven build

5)Build the parent project and then build the module-project

mvn clean install

6)Create an application.properties file on kafka-producer-wikimedia(PRODUCER) module

File - application.properties
-------------------------------

spring.kafka.producer.bootstrap-servers: localhost:9092
spring.kafka.producer.key-serializer: org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer: org.apache.kafka.common.serialization.StringSerializer


7)Create a Topic

File - TopicConfig.java
------------------------

package com.springboot.kafkaproducerwikimedia.kafkaConfig;

import org.apache.kafka.clients.admin.NewTopic;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.TopicBuilder;

@Configuration
public class TopicConfig {

    @Bean
    public NewTopic topic(){
        return TopicBuilder.name("wikimedia_recentchange_topic").build();
    }
}

7)Create a Producer class

File - wikimediaChnagesproducer.java

package com.springboot.kafkaproducerwikimedia.service;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class WikimediaChangesProducer {
    private static final Logger LOGGER = LoggerFactory.getLogger(WikimediaChangesProducer.class);
    private KafkaTemplate<String,String> kafkaTremplate;
    public WikimediaChangesProducer(KafkaTemplate<String,String> kafkaTremplate){
        this.kafkaTremplate = kafkaTremplate;
    }

    public void sendMessage(){

        String topic = "wikimedia_recentchange_topic";

        //in order to read real time stream data from wikimedia, we need event source

    }
}


8)
  >>> In order to create Event Source and read event message stream from the wikimedia , we would need to add few dependencies

  >>> google -> seach okhttp event source maven -> add the dependency on the pom.xml of module project (kafka-producer-wikimedia)

  <!-- https://mvnrepository.com/artifact/com.launchdarkly/okhttp-eventsource -->
  <dependency>
      <groupId>com.launchdarkly</groupId>
      <artifactId>okhttp-eventsource</artifactId>
      <version>2.5.0</version>
  </dependency>


  >> as wikimedia has data as JSON , so we would need dependency for JSON also and add it into pom.xml of module-project (kafka-producer-wikimedia)

  google -> search jackson json maven -> add the dependency on pom.xml of module-project (kafka-producer-wikimedia)

  <!-- https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-core -->
  <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-core</artifactId>
      <version>2.13.2</version>
  </dependency>

  <!-- https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind -->
  <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-databind</artifactId>
      <version>2.13.2.2</version>
  </dependency>


  <!-- https://mvnrepository.com/artifact/com.squareup.okhttp3/okhttp -->
  <dependency>
      <groupId>com.squareup.okhttp3</groupId>
      <artifactId>okhttp</artifactId>
      <version>4.9.3</version>
  </dependency>

  >> after that load the maven dependency




9)Create EventHadler for reading newly added event on the Wikimedia stream

  >>> after adding okhhttp , jackson dependency
  >>> we need to create a Handler method, when there will be a new event then Hanlder method will be triggered.

  Note =
    /*
       WikimediaChangesHandler , handler class will be triggered only there will be new event on the Wikimedia Stream
       we need to implement EventHandler from com.launchdarkly.eventsource.EventHandler package
       we need to override only onMessage() , from here kafkaTemplate will be triggered and event data will be send to Topic
       */

File - WikimediaChangesHandler.java (loc - kafka-producer-wikimedia microservice)
---------------------------------------

package com.springboot.kafkaproducerwikimedia.service;

import com.launchdarkly.eventsource.EventHandler;
import com.launchdarkly.eventsource.MessageEvent;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.core.KafkaTemplate;

public class WikimediaChangesHandler implements EventHandler {
    /*
     WikimediaChangesHandler , handler class will be triggered only there will be new event on the Wikimedia Stream
     we need to implement EventHandler from com.launchdarkly.eventsource.EventHandler package
     we need to override only onMessage() , from here kafkaTemplate will be triggered and event data will be send to Topic
     */
    private static final Logger LOGGER = LoggerFactory.getLogger(WikimediaChangesHandler.class);
    private String topic;
    private KafkaTemplate<String,String> kafkaTemplate;

    public WikimediaChangesHandler(String topic, KafkaTemplate<String, String> kafkaTemplate) {
        this.topic = topic;
        this.kafkaTemplate = kafkaTemplate;
    }

    @Override
    public void onOpen() throws Exception {

    }

    @Override
    public void onClosed() throws Exception {

    }

    @Override
    public void onMessage(String s, MessageEvent messageEvent) throws Exception {
       //OnMessage or EventHandler method will be triggered when there will be a new event on the wikimedia data stream
        LOGGER.info(String.format("event data -> %s", messageEvent.getData() ));
        kafkaTemplate.send(topic , messageEvent.getData());
    }

    @Override
    public void onComment(String s) throws Exception {

    }

    @Override
    public void onError(Throwable throwable) {

    }
}


10)
>>>> After creating the EventHandler class, next update the Producer class to call the EventHandler class and add the EventSource URL

  source url = https://stream.wikimedia.org/v2/stream/recentchange

  File - WikimediaChangesProducer.java
  --------------------------------------

  package com.springboot.kafkaproducerwikimedia.service;

  import com.launchdarkly.eventsource.EventHandler;
  import com.launchdarkly.eventsource.EventSource;
  import org.slf4j.Logger;
  import org.slf4j.LoggerFactory;
  import org.springframework.kafka.core.KafkaTemplate;
  import org.springframework.stereotype.Service;

  import java.net.URI;
  import java.util.concurrent.TimeUnit;

  @Service
  public class WikimediaChangesProducer {
      private static final Logger LOGGER = LoggerFactory.getLogger(WikimediaChangesProducer.class);
      private KafkaTemplate<String,String> kafkaTremplate;
      public WikimediaChangesProducer(KafkaTemplate<String,String> kafkaTremplate){
          this.kafkaTremplate = kafkaTremplate;
      }

      public void sendMessage() throws InterruptedException {

          String topic = "wikimedia_recentchange_topic";

          //in order to read real time stream data from wikimedia, we need event source
          EventHandler eventHandler = new WikimediaChangesHandler(topic,kafkaTremplate);
          String url= "https://stream.wikimedia.org/v2/stream/recentchange";
          EventSource.Builder builder = new EventSource.Builder(eventHandler, URI.create(url));
          EventSource eventSource = builder.build();
          eventSource.start();

          TimeUnit.MINUTES.sleep(10);

      }
  }


11)
>>> WikimediaChangesProducer.java class (Producer) has an Thread, run method need to implemnent on main method

package com.springboot.kafkaproducerwikimedia;

import com.springboot.kafkaproducerwikimedia.service.WikimediaChangesProducer;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class SpringbootProducerApplication implements CommandLineRunner {

    public static void main(String args[]){
        SpringApplication.run(SpringbootProducerApplication.class);
    }

    @Autowired
    private WikimediaChangesProducer wikimediaChangesProducer;

    @Override
    public void run(String... args) throws Exception {
        wikimediaChangesProducer.sendMessage();
    }
}

>>> run() method will be triggered when we run the application


**************************************************************************************************************************************************************************************************************************************************************************************************************************

SPRINGBOOT + KAFKA - REAL WORLD PROJECT - kafka-consumer-database (CONSUMER)
---------------------------------------------------------------------------------


1)Create a another module for Consumer on the Parent project

name = kafka-consumer-database

rightclick on the Parent project -> New module -> kafka-consumer-database

add <packaging>jar</packaging> tag on the module project (kafka-consumer-database)

mvn clean install


2)Create a main class on the kafka-consumer-database module

  package com.springboot.kafkaconsumerdatabase;

  import org.springframework.boot.SpringApplication;
  import org.springframework.boot.autoconfigure.SpringBootApplication;

  @SpringBootApplication
  public class SpringbootConsumerApplication {

      public static void main(String[] args) {
          SpringApplication.run(SpringbootConsumerApplication.class);
      }
  }


3)Create application.properties file on resources folder of module(kafka-consumer-database)

File - application.properties
------------------------------

spring.kafka.consumer.boostrap-servers: localhost:9092
spring.kafka.consumer.group-id: myGroup
spring.kafka.consumer.auto-offset-reset: earliest
spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer: org.apache.kafka.common.serialization.StringDeserializer


4)kafka consumer implementation

File - KafkaDatabaseConsumer.java
-------------------------------------

package com.springboot.kafkaconsumerdatabase.service;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class KafkaDatabaseConsumer {

    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaDatabaseConsumer.class);

    @KafkaListener(topics = "wikimedia_recentchange_topic" ,groupId = "myGroup")
    public void consume(String eventMessage){
        LOGGER.info(String.format("Event Message Receiveed -> %s", eventMessage));
    }

}


5) Run kafka Producer and Consumer and check the log info on console

Run -> run the entry point main method of both the modules (Producer and Consumer)


6)CONFIGURE MySQL DB (kafka-consumer-database) - CONSUMER module

=>>>Create a DB in MySQL workbench
create database wikimedia;

=>>>Add mysql driver properties and hibernate on the application.properties file

  File - application.properties
  -----------------------------

  #Springboot kafka consumer properties
  #
  spring.kafka.consumer.boostrap-servers: localhost:9092
  spring.kafka.consumer.group-id: myGroup
  spring.kafka.consumer.auto-offset-reset: earliest
  spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  spring.kafka.consumer.value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

  #
  #add mysql driver properties
  #
  spring.datasource.url=jdbc:mysql://localhost:3306/wikimedia?useSSL=false&serverTimezone=UTC
  spring.datasource.username=riyalogin
  spring.datasource.password=123456

  spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL8Dialect
  spring.jpa.hibernate.ddl-auto=update

  spring.jpa.properties.hibernate.show_sql=true
  spring.jpa.properties.hibernate.use_sql_comments=true
  spring.jpa.properties.hibernate.format_sql=true

  spring.kafka.topic.name=wikimedia_recentchange



=>>>Add dependency for Spring-Data-jpa and mysql-driver

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
        </dependency>

        <dependency>
            <groupId>com.mysql</groupId>
            <artifactId>mysql-connector-j</artifactId>
            <scope>runtime</scope>
        </dependency>


7)Save the wikimedia event data into DB(database)


>>>Create an entity - WikimediaData.java
  File - WikimediaData.java
  -------------------------
  package net.javaguides.springboot.entity;

  import lombok.Getter;
  import lombok.Setter;

  import javax.persistence.*;

  @Entity
  @Table(name = "wikimedia_recentchange")
  @Getter
  @Setter
  public class WikimediaData {

      @Id
      @GeneratedValue(strategy = GenerationType.IDENTITY)
      private Long id;

      @Lob
      private String wikiEventData;
  }

//@Lob annotation as wikimediadta string is large format


>>>Create a Repository interface that extends JpaRepositoryy

File - WikimediaDataRepository.java
-------------------------------------

package net.javaguides.springboot.repository;

import net.javaguides.springboot.entity.WikimediaData;
import org.springframework.data.jpa.repository.JpaRepository;

public interface WikimediaDataRepository extends JpaRepository<WikimediaData, Long> {
}



>>>update/inject Repository interface on the Consumer class in oreder to save the event data

File - KafkaDatabaseConsumer.java
----------------------------------

package net.javaguides.springboot;

import net.javaguides.springboot.entity.WikimediaData;
import net.javaguides.springboot.repository.WikimediaDataRepository;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class KafkaDatabaseConsumer {

    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaDatabaseConsumer.class);

    private WikimediaDataRepository dataRepository;

    public KafkaDatabaseConsumer(WikimediaDataRepository dataRepository) {
        this.dataRepository = dataRepository;
    }

    @KafkaListener(
            topics = "${spring.kafka.topic.name}",
            groupId = "${spring.kafka.consumer.group-id}"
    )
    public void consume(String eventMessage){

        LOGGER.info(String.format("Event message received -> %s", eventMessage));

        WikimediaData wikimediaData = new WikimediaData();
        wikimediaData.setWikiEventData(eventMessage);

        dataRepository.save(wikimediaData);
    }
}


**************************************************************************************************************************************************************************************************************************************************************************************************************************

EVENT DRIVEN - MICROSERVICE USING SPRINGBOOT and APACHE KAFKA
----------------------------------------------------------------

1)Create 4 microservices - order-service ,stock-service , Email-service and BaseDomains-service

2)Generate 4 Microservices using SpringInitializer

  >>> order-service
      [dependency - spring-web , Spring for Apache Kafka]

  >>> stock-service
      [dependency - spring-web , spring for Apache kafka]

  >>> email-service
      [dependency - spring-web , spring for Apache kafka]

  >>> base-domains
      [dependency - lombok]

  >>> On Intellij IDEA we can import only 1 project at a time, so in order to import all the projects together instead of having them on different Intellij window

  We create a folder named springboot-kafka-microservices as parent-folder project and paste all the 4 springboot projects into them
  and import the parent-folder project into Intellij IDEA 

  Note :-
  if we want to import extra microservice or project into the parent-folder project we can add it in the form of module
  File -> Project Structures -> Module


3)Change the tomacat port for order-service,stock-service and email-service

order-service = port 8080
stock-service = port 8081
email-service = port 8082


4)Create DTO in basics-domains microservice,
  >>> basic-domains microservice only contains DTO (only request, response Pojos)

  File - Order.java
  -------------------
  package net.javaguides.basedomains.dto;

  import lombok.AllArgsConstructor;
  import lombok.Data;
  import lombok.NoArgsConstructor;

  @Data
  @AllArgsConstructor
  @NoArgsConstructor
  public class Order {
      private String orderId;
      private String name;
      private int qty;
      private double price;
  }


  File - OrderEvent.java
  -----------------------
  package net.javaguides.basedomains.dto;

  import lombok.AllArgsConstructor;
  import lombok.Data;
  import lombok.NoArgsConstructor;

  @Data
  @AllArgsConstructor
  @NoArgsConstructor
  public class OrderEvent {
      private String message;
      private String status;
      private Order order;
  }



5)********    order-service microservice - PRODUCER (order-service)    ******
-------------------------------------------------------------------------------------

>>> Start the zookeeper and kafka-server

(1) 
>>> add spring.kafaka.producer properties and topic name on application.properties file
File - application.properties
------------------------------
spring.kafka.producer.bootstrap-servers: localhost:9092
spring.kafka.producer.key-serializer: org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
spring.kafka.topic.name=order_topics

(2)
>>> Create a Topic

File - KafkaTopicConfig.java
-----------------------------

package com.springboot.kafka.orderservice.config;

import org.apache.kafka.clients.admin.NewTopic;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.TopicBuilder;

@Configuration
public class KafkaTopicConfig {

    @Value("${spring.kafka.topic.name}")
    private String topicName;

    @Bean
    public NewTopic topic(){
        return TopicBuilder.name(topicName).build();
    }
}


(3) Create Kafka Producer

>>> order-service is the Producer and it sends the OrderEvent to the Kafka-broker/kafka-server
>>> orderevent and oder DTO/Pojo object is present in base-domains service
>>> base-domains is the different service and order-service is the different service
>>> order-service needs to use the DTO/object from the base-domains, so we need to add the base-domains dependency on the order-service

order-service (PRODUCER) (pom.xml)
-----------------------------------

    <dependency>
      <groupId>com.springboot.kafka</groupId>
      <artifactId>base-domains</artifactId>
      <version>0.0.1-SNAPSHOT</version>
    </dependency>

>>> create a package and class for producer class

File - OrderProducer.java
---------------------------

  package net.javaguides.orderservice.kafka;

  import net.javaguides.basedomains.dto.OrderEvent;
  import org.apache.kafka.clients.admin.NewTopic;
  import org.slf4j.Logger;
  import org.slf4j.LoggerFactory;
  import org.springframework.kafka.core.KafkaTemplate;
  import org.springframework.kafka.support.KafkaHeaders;
  import org.springframework.messaging.Message;
  import org.springframework.messaging.support.MessageBuilder;
  import org.springframework.stereotype.Service;

  @Service
  public class OrderProducer {

      private static final Logger LOGGER = LoggerFactory.getLogger(OrderProducer.class);

      private NewTopic topic;

      private KafkaTemplate<String, OrderEvent> kafkaTemplate;

      public OrderProducer(NewTopic topic, KafkaTemplate<String, OrderEvent> kafkaTemplate) {
          this.topic = topic;
          this.kafkaTemplate = kafkaTemplate;
      }

      public void sendMessage(OrderEvent event){
          LOGGER.info(String.format("Order event => %s", event.toString()));

          // create Message
          Message<OrderEvent> message = MessageBuilder
                  .withPayload(event)
                  .setHeader(KafkaHeaders.TOPIC, topic.name())
                  .build();
          kafkaTemplate.send(message);
      }
  }


(4)
>>> Create a Kafka Controller which takes the Request(Order) from the client and then send it to Producer
>>> Producer which is then send it to Kafka-server/kafka-broker

File - OrderController.java
----------------------------

package net.javaguides.orderservice.controller;

import net.javaguides.basedomains.dto.Order;
import net.javaguides.basedomains.dto.OrderEvent;
import net.javaguides.orderservice.kafka.OrderProducer;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.util.UUID;

@RestController
@RequestMapping("/api/v1")
public class OrderController {

    private OrderProducer orderProducer;

    public OrderController(OrderProducer orderProducer) {
        this.orderProducer = orderProducer;
    }

    @PostMapping("/orders")
    public String placeOrder(@RequestBody Order order){

        order.setOrderId(UUID.randomUUID().toString());

        OrderEvent orderEvent = new OrderEvent();
        orderEvent.setStatus("PENDING");
        orderEvent.setMessage("order status is in pending state");
        orderEvent.setOrder(order);

        orderProducer.sendMessage(orderEvent);

        return "Order placed successfully ...";
    }
}


6)*************  stock-service microservice (CONSUMER) - CONSUMER -  (stock-service) ********************
---------------------------------------------------------------------------------------------------------

(1)Update application.properties file to add the proprties details for the kafaka consumer

File - application.properties
-------------------------------
spring.kafka.consumer.bootstrap-servers: localhost:9092
spring.kafka.consumer.group-id: stock
spring.kafka.consumer.auto-offset-reset: earliest
spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.consumer.properties.spring.json.trusted.packages=*
spring.kafka.topic.name=order_topics

(2)Create a class for Consumer

File - OrderConsumer.java
-------------------------------

package net.javaguides.stockservice.kafka;

import net.javaguides.basedomains.dto.OrderEvent;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class OrderConsumer {

    private static final Logger LOGGER = LoggerFactory.getLogger(OrderConsumer.class);

    @KafkaListener(
            topics = "${spring.kafka.topic.name}"
            ,groupId = "${spring.kafka.consumer.group-id}"
    )
    public void consume(OrderEvent event){
        LOGGER.info(String.format("Order event received in stock service => %s", event.toString()));

        // save the order event into the database
    }
}


(3)Add dependency for base-domains microservice on stock-service pom.xml file

file - pom.xml (stock-service)
-------------------------------
   <dependency>
      <groupId>com.springboot.kafka</groupId>
      <artifactId>base-domains</artifactId>
      <version>0.0.1-SNAPSHOT</version>
    </dependency>



7) *********************  email-service microservice - email-service - CONSUMER *****************************
-----------------------------------------------------------------------------------------------------------------

(1)Update application.properties file to add the kafka.consumer properties

File - application.properties
------------------------------
server.port=8082
spring.kafka.consumer.bootstrap-servers: localhost:9092
spring.kafka.consumer.group-id: email
spring.kafka.consumer.auto-offset-reset: earliest
spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.consumer.properties.spring.json.trusted.packages=*
spring.kafka.topic.name=order_topics


(2)Create a class for consumer

File - OrderConsumer.java
--------------------------
package net.javaguides.emailservice.kafka;

import net.javaguides.basedomains.dto.OrderEvent;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class OrderConsumer {

    private static final Logger LOGGER = LoggerFactory.getLogger(OrderConsumer.class);

    @KafkaListener(
            topics = "${spring.kafka.topic.name}"
            ,groupId = "${spring.kafka.consumer.group-id}"
    )
    public void consume(OrderEvent event){
        LOGGER.info(String.format("Order event received in email service => %s", event.toString()));

        // send an email to the customer
    }
}


(3)add base-domains microservice dependency on the email-service

File - pom.xml (email-service)
--------------------------------

  <dependency>
      <groupId>com.springboot.kafka</groupId>
      <artifactId>base-domains</artifactId>
      <version>0.0.1-SNAPSHOT</version>
    </dependency>